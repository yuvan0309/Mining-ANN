# MACHINE LEARNING ALGORITHMS FOR SLOPE STABILITY PREDICTION

This document contains detailed algorithms for all models used in the slope stability prediction project.

---

## TABLE OF CONTENTS

1. [Gradient Boosting Regression](#1-gradient-boosting-regression)
2. [XGBoost (Extreme Gradient Boosting)](#2-xgboost-extreme-gradient-boosting)
3. [Random Forest Regression](#3-random-forest-regression)
4. [LightGBM (Light Gradient Boosting Machine)](#4-lightgbm-light-gradient-boosting-machine)
5. [Support Vector Machine (SVM)](#5-support-vector-machine-svm)
6. [Artificial Neural Network (ANN)](#6-artificial-neural-network-ann)

---

## 1. GRADIENT BOOSTING REGRESSION

**Performance**: RÂ² = 0.9426 (Test), 0.9954 (Train) | **Rank**: ğŸ¥‡ 1st

### ALGORITHM USED FOR GRADIENT BOOSTING REGRESSION

#### Step 1: Load and Prepare Data
1. Load the training dataset containing:
   - Features (X): Cohesion (c), Friction angle (Ï†), Unit weight (Î³), Pore pressure ratio (Ru)
   - Target (y): Factor of Safety (FoS)
2. Split data into training (80%) and test sets (20%)

#### Step 2: Initialize Hyperparameters
```
learning_rate = 0.1
n_estimators = 100
max_depth = 3
min_samples_split = 2
min_samples_leaf = 1
loss = 'squared_error'
random_state = 42
```

#### Step 3: Feature Scaling
1. Create StandardScaler instance
2. Fit scaler on training data:
   ```
   Î¼ = mean(X_train)
   Ïƒ = std(X_train)
   ```
3. Transform training and test data:
   ```
   X_scaled = (X - Î¼) / Ïƒ
   ```

#### Step 4: Initialize Base Prediction
1. Calculate initial prediction (mean of target):
   ```
   Fâ‚€(x) = È³ = (1/n) Â· Î£yáµ¢
   ```
2. Initialize residuals:
   ```
   ráµ¢ = yáµ¢ - Fâ‚€(x)
   ```

#### Step 5: Build Sequential Trees
**For each tree m = 1 to n_estimators:**

1. **Compute negative gradient (residuals):**
   ```
   ráµ¢â‚˜ = yáµ¢ - Fâ‚˜â‚‹â‚(xáµ¢)
   ```
   Where Fâ‚˜â‚‹â‚ is prediction from previous iteration

2. **Fit decision tree hâ‚˜(x) to residuals:**
   - Input: (X, ráµ¢â‚˜)
   - Constraints:
     - max_depth = 3 (tree depth)
     - min_samples_split = 2
     - min_samples_leaf = 1

3. **For each terminal node j in tree m:**
   Calculate optimal leaf value:
   ```
   Î³â±¼â‚˜ = argmin Î£ L(yáµ¢, Fâ‚˜â‚‹â‚(xáµ¢) + Î³)
   
   For squared error:
   Î³â±¼â‚˜ = mean(residuals in leaf j)
   ```

4. **Update model:**
   ```
   Fâ‚˜(x) = Fâ‚˜â‚‹â‚(x) + learning_rate Â· hâ‚˜(x)
   ```

#### Step 6: Prediction Phase
For new sample x:
```
Å· = Fâ‚€(x) + learning_rate Â· Î£â‚˜â‚Œâ‚á´¹ hâ‚˜(x)
```
Where M = n_estimators = 100

#### Step 7: Model Evaluation

1. **Calculate RÂ² Score:**
   ```
   SS_res = Î£(yáµ¢ - Å·áµ¢)Â²
   SS_tot = Î£(yáµ¢ - È³)Â²
   RÂ² = 1 - (SS_res / SS_tot)
   ```

2. **Calculate RMSE:**
   ```
   RMSE = âˆš[(1/n) Â· Î£(yáµ¢ - Å·áµ¢)Â²]
   ```

3. **Calculate MAE:**
   ```
   MAE = (1/n) Â· Î£|yáµ¢ - Å·áµ¢|
   ```

#### Step 8: Cross-Validation
1. Split data into k=5 folds
2. For each fold i:
   - Train on k-1 folds
   - Validate on fold i
   - Record RÂ² score
3. Compute mean CV score and standard deviation

#### Step 9: Save Model
1. Save trained model: `best_model_gradient_boosting.pkl`
2. Save scaler: `scaler.pkl`
3. Save metrics for comparison

---

## 2. XGBOOST (EXTREME GRADIENT BOOSTING)

**Performance**: RÂ² = 0.9420 (Test), 0.9581 (Train) | **Rank**: ğŸ¥ˆ 2nd

### ALGORITHM USED FOR XGBOOST REGRESSION

#### Step 1: Load and Prepare Data
1. Load the training dataset containing material properties and target FoS values
2. Separate features (X) from target variable (y)
3. Split data into training (80%) and test sets (20%)

#### Step 2: Initialize Fine-Tuned Hyperparameters
```python
n_estimators = 300      # Increased from 200 for better learning
max_depth = 6           # Reduced from 10 to prevent overfitting
learning_rate = 0.05    # Reduced from 0.1 for better generalization
subsample = 0.8         # Use 80% of samples per tree
colsample_bytree = 0.8  # Use 80% of features per tree
min_child_weight = 3    # Increased from 1 to prevent overfitting
gamma = 0.1             # Minimum loss reduction for split
reg_alpha = 0.1         # L1 regularization (LASSO)
reg_lambda = 1.0        # L2 regularization (Ridge)
random_state = 42
```

#### Step 3: Feature Scaling
1. Create StandardScaler instance
2. Fit scaler on training data and transform both sets

#### Step 4: Initialize Model
1. Create initial prediction (base value):
   ```
   Fâ‚€(x) = È³ = (1/n) Â· Î£yáµ¢
   ```
2. Set up regularized objective function:
   ```
   Obj = Î£ L(yáµ¢, Å·áµ¢) + Î£ Î©(fâ‚˜)
   
   Where:
   Î©(f) = Î³T + (Î»/2)Â·Î£wâ±¼Â² + Î±Â·Î£|wâ±¼|
   
   T = number of leaves
   wâ±¼ = leaf weights
   ```

#### Step 5: Build Boosted Trees (Regularized)
**For each tree m = 1 to n_estimators:**

1. **Compute first and second order gradients:**
   ```
   gáµ¢ = âˆ‚L(yáµ¢, Å·áµ¢â½áµâ»Â¹â¾) / âˆ‚Å·
   háµ¢ = âˆ‚Â²L(yáµ¢, Å·áµ¢â½áµâ»Â¹â¾) / âˆ‚Å·Â²
   
   For squared error:
   gáµ¢ = Å·áµ¢â½áµâ»Â¹â¾ - yáµ¢
   háµ¢ = 1
   ```

2. **Find best split for each node:**
   - Sample subsample fraction of data
   - Sample colsample_bytree fraction of features
   - For each feature:
     ```
     Gain = (Î£gáµ¢)Â² / (Î£háµ¢ + Î») - Î³
     ```
   - Choose split with maximum gain > gamma

3. **Calculate optimal leaf weights:**
   ```
   wâ±¼* = -(Î£gáµ¢) / (Î£háµ¢ + Î»)
   ```

4. **Update predictions:**
   ```
   Å·áµ¢â½áµâ¾ = Å·áµ¢â½áµâ»Â¹â¾ + Î· Â· wâ±¼*
   ```
   Where Î· = learning_rate = 0.05

#### Step 6: Prediction Phase
For each new sample x:
```
Å· = Fâ‚€(x) + Î· Â· Î£â‚˜â‚Œâ‚á´¹ hâ‚˜(x)
```
Where M = n_estimators = 300

#### Step 7: Regularization Benefits
1. **Tree Complexity Penalty (Î³)**: Prevents creating too many leaves
2. **L2 Regularization (Î»)**: Smooths leaf weights
3. **L1 Regularization (Î±)**: Promotes sparsity
4. **Subsampling**: Prevents overfitting by using subset of data
5. **Column Sampling**: Reduces feature correlation

#### Step 8: Model Evaluation
Calculate RÂ², RMSE, MAE on test set

#### Step 9: Cross-Validation
5-fold cross-validation to assess generalization

#### Step 10: Save Model
Save as `best_model_xgboost.pkl`

---

## 3. RANDOM FOREST REGRESSION

**Performance**: RÂ² = 0.9924 (Train) | **Rank**: ğŸ¥ˆ 2nd (Training)

### ALGORITHM USED FOR RANDOM FOREST REGRESSION

#### Step 1: Load and Prepare Data
1. Load the training and test data
2. Separate features (X) from target (y)

#### Step 2: Initialize Hyperparameters
```
n_estimators = 200    # Number of trees in forest
max_depth = 15        # Maximum depth of each tree
min_samples_split = 2 # Minimum samples to split node
min_samples_leaf = 1  # Minimum samples in leaf
random_state = 42
```

#### Step 3: Feature Scaling
Apply StandardScaler to normalize features

#### Step 4: Build Random Forest
**For each tree t = 1 to n_estimators:**

1. **Create Bootstrap Sample:**
   ```
   Sample n observations with replacement
   Bootstrap_t = random_sample(X_train, n, replace=True)
   ```

2. **Start with all bootstrap samples at root node**

3. **For each node to split:**
   
   a. **Randomly select m features:**
      ```
      m = âˆš(number_of_features)  # For regression
      features_subset = random_sample(features, m)
      ```
   
   b. **Find best split using RMSE criterion:**
      ```
      For each feature f in features_subset:
        For each threshold t:
          Split data: left = X[f â‰¤ t], right = X[f > t]
          
          Calculate weighted RMSE:
          RMSE = (n_left/n)Â·RMSE_left + (n_right/n)Â·RMSE_right
          
      Choose split with minimum RMSE
      ```
   
   c. **Split the data into child nodes**

4. **Stop splitting when:**
   - Maximum depth reached (max_depth = 15)
   - Node has < min_samples_split samples
   - All samples have same target value
   - Pure leaf node created

5. **Assign leaf node predictions:**
   ```
   prediction = mean(y_samples_in_leaf)
   ```

#### Step 5: Prediction Phase
For new sample x:

1. **Pass through all trees:**
   ```
   predictions = [tree_t.predict(x) for t in 1 to n_estimators]
   ```

2. **Compute average prediction:**
   ```
   Å· = (1/n_estimators) Â· Î£predictions
   ```

#### Step 6: Feature Importance
Calculate importance based on reduction in RMSE:
```
For each feature f:
  importance_f = Î£ (RMSE_before_split - RMSE_after_split)
  
Normalize so Î£ importance = 1
```

#### Step 7: Model Evaluation
1. Calculate RÂ² Score
2. Calculate RMSE
3. Calculate MAE
4. Perform 5-fold cross-validation

#### Step 8: End
Model trained and ready for deployment

---

## 4. LIGHTGBM (LIGHT GRADIENT BOOSTING MACHINE)

**Performance**: RÂ² = 0.9872 (Train) | **Rank**: ğŸ¥‰ 3rd

### ALGORITHM USED FOR LIGHTGBM REGRESSION

#### Step 1: Load and Prepare Data
1. Load training and test datasets
2. Separate features (X) from target (y)
3. Split into 80% training, 20% test

#### Step 2: Initialize Hyperparameters
```python
num_leaves = 31           # Maximum tree leaves
learning_rate = 0.05      # Shrinkage rate
n_estimators = 100        # Number of boosting rounds
max_depth = -1            # No limit (-1)
min_child_samples = 20    # Minimum samples in leaf
subsample = 0.8           # Row sampling ratio
colsample_bytree = 0.8    # Column sampling ratio
random_state = 42
```

#### Step 3: Feature Scaling
Apply StandardScaler to features

#### Step 4: Initialize Base Prediction
```
Fâ‚€(x) = È³ = mean(y_train)
```

#### Step 5: Build Gradient Boosting Trees (Leaf-wise)
**For each iteration m = 1 to n_estimators:**

1. **Compute gradients:**
   ```
   gáµ¢ = âˆ‚L(yáµ¢, Å·áµ¢) / âˆ‚Å· = Å·áµ¢ - yáµ¢
   háµ¢ = âˆ‚Â²L(yáµ¢, Å·áµ¢) / âˆ‚Å·Â² = 1
   ```

2. **Build histogram-based tree:**
   
   a. **Bin continuous features into histograms**
   
   b. **Find best split using Gradient-based One-Side Sampling (GOSS):**
      - Keep all large gradient samples
      - Randomly sample small gradient samples
      - Calculate gain for each split candidate
   
   c. **Split criterion (best leaf-wise split):**
      ```
      Gain = (Î£gáµ¢_left)Â² / (Î£háµ¢_left) + (Î£gáµ¢_right)Â² / (Î£háµ¢_right) - (Î£gáµ¢)Â² / (Î£háµ¢)
      ```
   
   d. **Grow tree leaf-wise (not level-wise):**
      - Always split the leaf with maximum gain
      - Stop when num_leaves reached or min_child_samples violated

3. **Calculate optimal leaf weights:**
   ```
   wâ±¼ = -(Î£gáµ¢) / (Î£háµ¢ + Î»)
   ```

4. **Update predictions:**
   ```
   Fâ‚˜(x) = Fâ‚˜â‚‹â‚(x) + learning_rate Â· hâ‚˜(x)
   ```

#### Step 6: Exclusive Feature Bundling (EFB)
LightGBM bundles mutually exclusive features to reduce dimensions

#### Step 7: Prediction Phase
For new sample x:
```
Å· = Fâ‚€(x) + learning_rate Â· Î£â‚˜â‚Œâ‚á´¹ hâ‚˜(x)
```

#### Step 8: Model Evaluation
Calculate RÂ², RMSE, MAE on test set

#### Step 9: Cross-Validation
Perform 5-fold cross-validation

#### Step 10: End
Model ready for predictions

---

## 5. SUPPORT VECTOR MACHINE (SVM)

**Performance**: RÂ² = 0.9570 (Train) | **Rank**: 5th

### ALGORITHM USED FOR SVM REGRESSION (SVR)

#### Step 1: Load and Prepare Data
1. Load training and test datasets
2. Separate features (X) from target (y)

#### Step 2: Initialize Hyperparameters
```python
kernel = 'rbf'      # Radial Basis Function
C = 100             # Regularization parameter
gamma = 'scale'     # Kernel coefficient (1/(n_features * X.var()))
epsilon = 0.1       # Epsilon-tube width
```

#### Step 3: Feature Scaling
**Critical for SVM - must scale features:**
```
X_scaled = (X - Î¼) / Ïƒ
```

#### Step 4: Define RBF Kernel Function
```
K(x, x') = exp(-Î³ Â· ||x - x'||Â²)

Where:
Î³ = 1 / (n_features Â· var(X))  # For gamma='scale'
```

#### Step 5: Solve Îµ-SVR Optimization Problem

**Primal Form:**
```
minimize: (1/2)||w||Â² + C Â· Î£(Î¾áµ¢ + Î¾áµ¢*)

subject to:
  yáµ¢ - (wÂ·Ï†(xáµ¢) + b) â‰¤ Îµ + Î¾áµ¢
  (wÂ·Ï†(xáµ¢) + b) - yáµ¢ â‰¤ Îµ + Î¾áµ¢*
  Î¾áµ¢, Î¾áµ¢* â‰¥ 0
```

Where:
- w = weight vector in feature space
- Ï†(x) = feature mapping (implicit via kernel)
- b = bias term
- Îµ = epsilon (insensitivity tube)
- Î¾áµ¢, Î¾áµ¢* = slack variables
- C = penalty for violations

**Dual Form (solved in practice):**
```
maximize: Î£yáµ¢(Î±áµ¢ - Î±áµ¢*) - ÎµÂ·Î£(Î±áµ¢ + Î±áµ¢*) - (1/2)Â·Î£Î£(Î±áµ¢ - Î±áµ¢*)(Î±â±¼ - Î±â±¼*)K(xáµ¢, xâ±¼)

subject to:
  0 â‰¤ Î±áµ¢, Î±áµ¢* â‰¤ C
  Î£(Î±áµ¢ - Î±áµ¢*) = 0
```

#### Step 6: Identify Support Vectors
Support vectors are samples where:
```
Î±áµ¢ > 0 or Î±áµ¢* > 0
```

These are samples on or outside the Îµ-tube

#### Step 7: Prediction Phase
For new sample x:
```
Å· = Î£(Î±áµ¢ - Î±áµ¢*) Â· K(xáµ¢, x) + b

Where sum is only over support vectors
```

#### Step 8: Model Evaluation
1. Calculate RÂ² Score
2. Calculate RMSE
3. Calculate MAE
4. Perform cross-validation

#### Step 9: End
SVM model trained and ready

---

## 6. ARTIFICIAL NEURAL NETWORK (ANN)

**Performance**: RÂ² = 0.9316 (Train) | **Rank**: 6th

### ALGORITHM USED FOR ANN REGRESSION (MLP)

#### Step 1: Load and Prepare Data
1. Load training and test datasets
2. Separate features (X) from target (y)

#### Step 2: Initialize Network Architecture
```python
hidden_layer_sizes = (64, 32, 16)  # 3 hidden layers
activation = 'relu'                 # ReLU activation
solver = 'adam'                     # Adam optimizer
learning_rate_init = 0.001         # Initial learning rate
max_iter = 1000                    # Maximum epochs
random_state = 42
```

**Network Structure:**
```
Input Layer:    4 neurons (c, Ï†, Î³, Ru)
Hidden Layer 1: 64 neurons + ReLU
Hidden Layer 2: 32 neurons + ReLU
Hidden Layer 3: 16 neurons + ReLU
Output Layer:   1 neuron (FoS)
```

#### Step 3: Feature Scaling
```
X_scaled = (X - Î¼) / Ïƒ
```

#### Step 4: Initialize Weights and Biases
For each layer l:
```
Wâ½Ë¡â¾ ~ N(0, âˆš(2/náµ¢â‚™))  # He initialization
bâ½Ë¡â¾ = 0
```

#### Step 5: Forward Propagation

**For each training sample x:**

1. **Input to Hidden Layer 1:**
   ```
   zâ½Â¹â¾ = Wâ½Â¹â¾Â·x + bâ½Â¹â¾
   aâ½Â¹â¾ = ReLU(zâ½Â¹â¾) = max(0, zâ½Â¹â¾)
   ```

2. **Hidden Layer 1 to Hidden Layer 2:**
   ```
   zâ½Â²â¾ = Wâ½Â²â¾Â·aâ½Â¹â¾ + bâ½Â²â¾
   aâ½Â²â¾ = ReLU(zâ½Â²â¾)
   ```

3. **Hidden Layer 2 to Hidden Layer 3:**
   ```
   zâ½Â³â¾ = Wâ½Â³â¾Â·aâ½Â²â¾ + bâ½Â³â¾
   aâ½Â³â¾ = ReLU(zâ½Â³â¾)
   ```

4. **Hidden Layer 3 to Output:**
   ```
   zâ½â´â¾ = Wâ½â´â¾Â·aâ½Â³â¾ + bâ½â´â¾
   Å· = zâ½â´â¾  # Linear activation for regression
   ```

#### Step 6: Compute Loss
```
L = (1/2n) Â· Î£(yáµ¢ - Å·áµ¢)Â²  # Mean Squared Error
```

#### Step 7: Backward Propagation (Gradient Computation)

1. **Output layer gradient:**
   ```
   Î´â½â´â¾ = Å· - y
   ```

2. **Hidden layer 3 gradient:**
   ```
   Î´â½Â³â¾ = (Wâ½â´â¾)áµ€Â·Î´â½â´â¾ âŠ™ ReLU'(zâ½Â³â¾)
   
   Where ReLU'(z) = 1 if z > 0, else 0
   ```

3. **Hidden layer 2 gradient:**
   ```
   Î´â½Â²â¾ = (Wâ½Â³â¾)áµ€Â·Î´â½Â³â¾ âŠ™ ReLU'(zâ½Â²â¾)
   ```

4. **Hidden layer 1 gradient:**
   ```
   Î´â½Â¹â¾ = (Wâ½Â²â¾)áµ€Â·Î´â½Â²â¾ âŠ™ ReLU'(zâ½Â¹â¾)
   ```

5. **Compute weight and bias gradients:**
   ```
   âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾ Â· (aâ½Ë¡â»Â¹â¾)áµ€
   âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾
   ```

#### Step 8: Update Weights using Adam Optimizer

For each parameter Î¸ (weights and biases):

1. **Compute momentum (first moment):**
   ```
   m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t
   
   Where:
   g_t = âˆ‚L/âˆ‚Î¸
   Î²â‚ = 0.9
   ```

2. **Compute velocity (second moment):**
   ```
   v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²
   
   Where Î²â‚‚ = 0.999
   ```

3. **Bias correction:**
   ```
   mÌ‚_t = m_t / (1 - Î²â‚áµ—)
   vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)
   ```

4. **Update parameters:**
   ```
   Î¸_t = Î¸_{t-1} - Î±Â·mÌ‚_t / (âˆšvÌ‚_t + Îµ)
   
   Where:
   Î± = learning_rate = 0.001
   Îµ = 10â»â¸
   ```

#### Step 9: Iterate
Repeat Steps 5-8 for max_iter epochs or until convergence

#### Step 10: Prediction Phase
For new sample x:
1. Apply forward propagation (Step 5)
2. Return output Å·

#### Step 11: Model Evaluation
Calculate RÂ², RMSE, MAE on test set

#### Step 12: End
Neural network trained and ready

---

## MODEL COMPARISON SUMMARY

| Model | Algorithm Type | Test RÂ² | Train RÂ² | RMSE | MAE | Overfitting Gap |
|-------|---------------|---------|----------|------|-----|-----------------|
| **Gradient Boosting** | Sequential Ensemble | **94.26%** | 99.54% | 0.0834 | **0.0563** | 5.28% |
| **XGBoost** | Regularized Boosting | 94.20% | 95.81% | 0.0838 | 0.0597 | **1.61%** âœ“ |
| **Random Forest** | Parallel Ensemble | - | 99.24% | 0.0313 | 0.0220 | Not tested |
| **LightGBM** | Leaf-wise Boosting | - | 98.72% | 0.0407 | 0.0297 | Not tested |
| **SVM** | Kernel Method | - | 95.70% | 0.0746 | 0.0616 | Not tested |
| **ANN** | Neural Network | - | 93.16% | 0.0940 | 0.0694 | Not tested |

---

## KEY INSIGHTS

### Why Gradient Boosting Won:
âœ… Highest test accuracy (94.26%)  
âœ… Lowest MAE (0.0563)  
âœ… Best overall performance  

### Why XGBoost is Close Second:
âœ… Best generalization (1.61% gap)  
âœ… Regularization prevents overfitting  
âœ… Nearly identical test RÂ² (94.20%)  
âœ… Production-ready with robust predictions  

### Production Deployment:
Both Gradient Boosting and XGBoost are deployed in the web application for ensemble predictions.

---

**Generated**: November 2025  
**Project**: Slope Stability Prediction using Machine Learning  
**Method**: Bishop's Simplified Method  
**Dataset**: 361 samples (80% train, 20% test)  
**Validation**: 5-fold cross-validation  

---
